{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum k-cut Algorithm\n",
    "\n",
    "### High-level Algorithm Specification:\n",
    "\n",
    "1. Create vertex list and an edges list, e.g.:\n",
    "\n",
    "    ```javascript\n",
    "    vertices = {1: [2,4,5], 2: [3,4,5], 3: [2,4], 4: [1,2,3], 5: [1,2]}\n",
    "    edges = [[1,2], [1,4], [1,5], [2,3], [2,4], [2,5], [3,4]]\n",
    "    ```\n",
    "\n",
    "2. Keep track of the minimum cut so far:\n",
    "\n",
    "    ```javascript\n",
    "    // really this could be the max degree of all vertices, I believe\n",
    "    min_edges_so_far = len(edges)\n",
    "    min_vertex_sets = {1:[], 2:[], 3:[], 4:[], 5:[]}\n",
    "    ```\n",
    "\n",
    "3. *Iterate at least `n^2 log n` times (where n is the original number of vertices)*\n",
    "    \n",
    "    **Intiate:**\n",
    "    \n",
    "    ```javascript\n",
    "    temp_vertex_sets = copy(min_vertex_sets)\n",
    "    temp_vertices = copy(vertices)\n",
    "    temp_edges = copy(edges)\n",
    "    ```\n",
    "\n",
    "    **While num_vertices > k:**\n",
    "\n",
    "    1. Pick an edge at random: the first vertex (`v1`) will absorb the second (`v2`). Add `v2` and `temp_vertex_sets[v2]` to `temp_vertex_sets[v1]` and delete `temp_vertex_sets[v2]`.\n",
    "    2. All vertices adjacent to `v2` are added to `temp_vertices[v1]` unless already present. Remove `v2` from `temp_vertices[v1]`.\n",
    "    3. Replace all instances of `v2` in `temp_edges` with `v1`, unless the other vertex of the edge is itself `v1`. In the latter case, delete the edge (e.g. remove self-loops). **Note:** Parallel edges are allowed; there may be multiple instances of an edge comprised the same vertex pair.\n",
    "\n",
    "    **Finally:** The number of final edges is the number of edges across the final cut in this iteration. If it is less than min_edges_so_far, update `min_edges_so_far = len(temp_edges)` and `min_vertex_sets = temp_vertex_sets`.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Select all measurements and document ids from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "database = 'fomc'\n",
    "conn = psycopg2.connect(\"dbname=\" + database + \" user=abarciauskas\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# year = 2006\n",
    "# cosine_thresh = 0.25\n",
    "# limit = 1000\n",
    "# cur.execute(\"SELECT Doc1Id,Doc2Id,CosineSimilarity FROM alignments WHERE Year = '\" + str(year) + \"'\"\n",
    "#            \" AND CosineSimilarity >= \" + str(cosine_thresh) + \" ORDER BY random() LIMIT \" + str(limit))\n",
    "# cosine_sims = cur.fetchall()\n",
    "# len(cosine_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the graph\n",
    "\n",
    "The graph is comprised a list of edges (a vertex tuple) and a dictionary of vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "def create_graph(alignments):\n",
    "    edges = [tuple([x[0],x[1]]) for x in alignments]\n",
    "    vertices = {}\n",
    "    for edge in edges:\n",
    "        v1 = edge[0]\n",
    "        v2 = edge[1]\n",
    "        if v1 in vertices.keys():\n",
    "            vertices[v1].add(v2)\n",
    "        else:\n",
    "            vertices[v1] = {v2}\n",
    "        if v2 in vertices.keys():\n",
    "            vertices[v2].add(v1)\n",
    "        else:\n",
    "            vertices[v2] = {v1}\n",
    "    return [edges, vertices]\n",
    "\n",
    "edges, vertices = create_graph(cosine_sims)\n",
    "print len(vertices)\n",
    "print len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number vertices fully connected graph: 506\n"
     ]
    }
   ],
   "source": [
    "# need to find disconnected graphs\n",
    "graphs = []\n",
    "unvisited = set(vertices.keys())\n",
    "\n",
    "#Detect distinct graphs\n",
    "while len(unvisited) > 0:\n",
    "    # for every vertex, find all of its connected components and recurse on those vertices\n",
    "    visited = []\n",
    "    current_vertex = unvisited.pop()\n",
    "    visited.append(current_vertex)\n",
    "    stack_to_visit = list(vertices[current_vertex])\n",
    "    while len(stack_to_visit) > 0:\n",
    "        current_vertex = stack_to_visit.pop()\n",
    "        current_adj_vtcs = vertices[current_vertex]\n",
    "        if current_vertex not in visited: visited.append(current_vertex)\n",
    "        if current_vertex in unvisited: unvisited.remove(current_vertex)        \n",
    "        for v in current_adj_vtcs:\n",
    "            if v not in visited:\n",
    "                stack_to_visit.insert(0, v)\n",
    "    graphs.append(visited)\n",
    "\n",
    "# print len(unvisited)\n",
    "# print len(visited)\n",
    "# print len(graphs)\n",
    "# print ''\n",
    "\n",
    "graph_lengths = [len(graph) for graph in graphs]\n",
    "fc_graph = graphs[graph_lengths.index(max(graph_lengths))]\n",
    "print 'Number vertices fully connected graph: ' + str(len(fc_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772\n",
      "506\n",
      "1000\n",
      "726\n"
     ]
    }
   ],
   "source": [
    "# Remove loner graphs\n",
    "# fc = fully connected\n",
    "set_fc_graph_vertices = set(fc_graph)\n",
    "loners = set_fc_graph_vertices ^ set(vertices.keys())\n",
    "\n",
    "fc_vertices = deepcopy(vertices)\n",
    "fc_edges = deepcopy(edges)\n",
    "\n",
    "print(len(fc_vertices))\n",
    "for loner in loners:\n",
    "    fc_vertices.pop(loner, None)\n",
    "print(len(fc_vertices))\n",
    "\n",
    "print(len(fc_edges))\n",
    "fc_edges = filter(lambda x: not list(x)[0] in loners and not list(x)[1] in loners, fc_edges)    \n",
    "print(len(fc_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Keep track of minimum so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Random iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def run(niters):\n",
    "    min_fc_edges_so_far = len(fc_edges)\n",
    "    min_vertex_sets = {key: set() for key in fc_vertices.keys()}\n",
    "    for iteridx in range(niters):\n",
    "        if iteridx % 50 == 0: print 'Running iter: ' + str(iteridx)        \n",
    "        temp_vertex_sets = {key: set() for key in fc_vertices.keys()}\n",
    "        temp_fc_vertices = deepcopy(fc_vertices)\n",
    "        temp_fc_edges = fc_edges[:]        \n",
    "        while len(temp_fc_vertices) > k:\n",
    "            # pick an edge at random and delete it\n",
    "            rand_idx = int(random.random()*len(temp_fc_edges))\n",
    "            random_edge = temp_fc_edges.pop(rand_idx)\n",
    "            # Add v2 and temp_vertex_sets[v2] to temp_vertex_sets[v1] and delete temp_vertex_sets[v2]\n",
    "            v1 = list(random_edge)[0]\n",
    "            v2 = list(random_edge)[1]\n",
    "            temp_vertex_sets[v1] = temp_vertex_sets[v1].union(temp_vertex_sets[v2])\n",
    "            temp_vertex_sets[v1].add(v2)\n",
    "            temp_vertex_sets.pop(v2, None)\n",
    "\n",
    "            # All fc_vertices adjacent to v2 are added to temp_fc_vertices[v1] unless already present.\n",
    "            # Remove v2 from temp_fc_vertices[v1].\n",
    "            adj_v2 = temp_fc_vertices[v2]\n",
    "            temp_fc_vertices[v1] = temp_fc_vertices[v1].union(adj_v2)\n",
    "            temp_fc_vertices[v1].remove(v2)\n",
    "            temp_fc_vertices.pop(v2, None)\n",
    "\n",
    "            # Replace all instances of v2 in temp_fc_edges with v1, unless the other vertex of the edge is itself v1.\n",
    "            # In the latter case, delete the edge (e.g. remove self-loops).\n",
    "            # Note: Parallel fc_edges are allowed; there may be multiple instances of an edge comprised the same vertex pair.\n",
    "            remove_fc_edges = []\n",
    "            for i,cur_edge in enumerate(temp_fc_edges):\n",
    "                if len(cur_edge) > 1:\n",
    "                    cur_edge_v1 = list(cur_edge)[0]\n",
    "                    cur_edge_v2 = list(cur_edge)[1]\n",
    "                    if (cur_edge == random_edge):\n",
    "                        remove_fc_edges.append(i)\n",
    "                    elif cur_edge_v1 == v2:\n",
    "                        temp_fc_edges[i] = {v1, cur_edge_v2}\n",
    "                        # remove this edge from temp_fc_vertices\n",
    "                        # it may have already been removed because we keep parallel fc_edges around\n",
    "                        if v2 in temp_fc_vertices[cur_edge_v2]: temp_fc_vertices[cur_edge_v2].remove(v2)\n",
    "                    elif cur_edge_v2 == v2:\n",
    "                        temp_fc_edges[i] = {cur_edge_v1, v1}\n",
    "                        # it may have already been removed because we keep parallel fc_edges around\n",
    "                        if v2 in temp_fc_vertices[cur_edge_v1]: temp_fc_vertices[cur_edge_v1].remove(v2)\n",
    "            # work around for delete\n",
    "            temp_fc_edges = [set(i) for j, i in enumerate(temp_fc_edges) if j not in remove_fc_edges]\n",
    "            #Finally: The number of final fc_edges is the number of fc_edges across the final cut in this iteration.\n",
    "            #If it is less than min_fc_edges_so_far, update min_fc_edges_so_far = len(temp_fc_edges) and min_vertex_sets = temp_vertex_sets.\n",
    "            if len(temp_fc_edges) < min_fc_edges_so_far:\n",
    "                min_fc_edges_so_far = len(temp_fc_edges)\n",
    "                min_vertex_sets = temp_vertex_sets\n",
    "    return min_fc_edges_so_far, min_vertex_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iter: 0\n",
      "Total time for 1: 0.264104127884\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "n = len(fc_vertices)\n",
    "niters = 1\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "min_fc_edges_so_far, min_vertex_sets = run(niters)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ': ' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733622577455\n"
     ]
    }
   ],
   "source": [
    "niters = n**2#int(np.ceil(n**2*np.log(n)))\n",
    "niters = 10000\n",
    "total_seconds = niters*total\n",
    "minutes = total_seconds/60\n",
    "hours = minutes/60\n",
    "print hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iter: 0\n",
      "Running iter: 50\n",
      "Running iter: 100\n",
      "Running iter: 150\n",
      "Running iter: 200\n",
      "Running iter: 250\n",
      "Running iter: 300\n",
      "Running iter: 350\n",
      "Running iter: 400\n",
      "Running iter: 450\n",
      "Running iter: 500\n",
      "Running iter: 550\n",
      "Running iter: 600\n",
      "Running iter: 650\n",
      "Running iter: 700\n",
      "Running iter: 750\n",
      "Running iter: 800\n",
      "Running iter: 850\n",
      "Running iter: 900\n",
      "Running iter: 950\n",
      "Running iter: 1000\n",
      "Running iter: 1050\n",
      "Running iter: 1100\n",
      "Running iter: 1150\n",
      "Running iter: 1200\n",
      "Running iter: 1250\n",
      "Running iter: 1300\n",
      "Running iter: 1350\n",
      "Running iter: 1400\n",
      "Running iter: 1450\n",
      "Running iter: 1500\n",
      "Running iter: 1550\n",
      "Running iter: 1600\n",
      "Running iter: 1650\n",
      "Running iter: 1700\n",
      "Running iter: 1750\n",
      "Running iter: 1800\n",
      "Running iter: 1850\n",
      "Running iter: 1900\n",
      "Running iter: 1950\n",
      "Running iter: 2000\n",
      "Running iter: 2050\n",
      "Running iter: 2100\n",
      "Running iter: 2150\n",
      "Running iter: 2200\n",
      "Running iter: 2250\n",
      "Running iter: 2300\n",
      "Running iter: 2350\n",
      "Running iter: 2400\n",
      "Running iter: 2450\n",
      "Running iter: 2500\n",
      "Running iter: 2550\n",
      "Running iter: 2600\n",
      "Running iter: 2650\n",
      "Running iter: 2700\n",
      "Running iter: 2750\n",
      "Running iter: 2800\n",
      "Running iter: 2850\n",
      "Running iter: 2900\n",
      "Running iter: 2950\n",
      "Running iter: 3000\n",
      "Running iter: 3050\n",
      "Running iter: 3100\n",
      "Running iter: 3150\n",
      "Running iter: 3200\n",
      "Running iter: 3250\n",
      "Running iter: 3300\n",
      "Running iter: 3350\n",
      "Running iter: 3400\n",
      "Running iter: 3450\n",
      "Running iter: 3500\n",
      "Running iter: 3550\n",
      "Running iter: 3600\n",
      "Running iter: 3650\n",
      "Running iter: 3700\n",
      "Running iter: 3750\n",
      "Running iter: 3800\n",
      "Running iter: 3850\n",
      "Running iter: 3900\n",
      "Running iter: 3950\n",
      "Running iter: 4000\n",
      "Running iter: 4050\n",
      "Running iter: 4100\n",
      "Running iter: 4150\n",
      "Running iter: 4200\n",
      "Running iter: 4250\n",
      "Running iter: 4300\n",
      "Running iter: 4350\n",
      "Running iter: 4400\n",
      "Running iter: 4450\n",
      "Running iter: 4500\n",
      "Running iter: 4550\n",
      "Running iter: 4600\n",
      "Running iter: 4650\n",
      "Running iter: 4700\n",
      "Running iter: 4750\n",
      "Running iter: 4800\n",
      "Running iter: 4850\n",
      "Running iter: 4900\n",
      "Running iter: 4950\n",
      "Running iter: 5000\n",
      "Running iter: 5050\n",
      "Running iter: 5100\n",
      "Running iter: 5150\n",
      "Running iter: 5200\n",
      "Running iter: 5250\n",
      "Running iter: 5300\n",
      "Running iter: 5350\n",
      "Running iter: 5400\n",
      "Running iter: 5450\n",
      "Running iter: 5500\n",
      "Running iter: 5550\n",
      "Running iter: 5600\n",
      "Running iter: 5650\n",
      "Running iter: 5700\n",
      "Running iter: 5750\n",
      "Running iter: 5800\n",
      "Running iter: 5850\n",
      "Running iter: 5900\n",
      "Running iter: 5950\n",
      "Running iter: 6000\n",
      "Running iter: 6050\n",
      "Running iter: 6100\n",
      "Running iter: 6150\n",
      "Running iter: 6200\n",
      "Running iter: 6250\n",
      "Running iter: 6300\n",
      "Running iter: 6350\n",
      "Running iter: 6400\n",
      "Running iter: 6450\n",
      "Running iter: 6500\n",
      "Running iter: 6550\n",
      "Running iter: 6600\n",
      "Running iter: 6650\n",
      "Running iter: 6700\n",
      "Running iter: 6750\n",
      "Running iter: 6800\n",
      "Running iter: 6850\n",
      "Running iter: 6900\n",
      "Running iter: 6950\n",
      "Running iter: 7000\n",
      "Running iter: 7050\n",
      "Running iter: 7100\n",
      "Running iter: 7150\n",
      "Running iter: 7200\n",
      "Running iter: 7250\n",
      "Running iter: 7300\n",
      "Running iter: 7350\n",
      "Running iter: 7400\n",
      "Running iter: 7450\n",
      "Running iter: 7500\n",
      "Running iter: 7550\n",
      "Running iter: 7600\n",
      "Running iter: 7650\n",
      "Running iter: 7700\n",
      "Running iter: 7750\n",
      "Running iter: 7800\n",
      "Running iter: 7850\n",
      "Running iter: 7900\n",
      "Running iter: 7950\n",
      "Running iter: 8000\n",
      "Running iter: 8050\n",
      "Running iter: 8100\n",
      "Running iter: 8150\n",
      "Running iter: 8200\n",
      "Running iter: 8250\n",
      "Running iter: 8300\n",
      "Running iter: 8350\n",
      "Running iter: 8400\n",
      "Running iter: 8450\n",
      "Running iter: 8500\n",
      "Running iter: 8550\n",
      "Running iter: 8600\n",
      "Running iter: 8650\n",
      "Running iter: 8700\n",
      "Running iter: 8750\n",
      "Running iter: 8800\n",
      "Running iter: 8850\n",
      "Running iter: 8900\n",
      "Running iter: 8950\n",
      "Running iter: 9000\n",
      "Running iter: 9050\n",
      "Running iter: 9100\n",
      "Running iter: 9150\n",
      "Running iter: 9200\n",
      "Running iter: 9250\n",
      "Running iter: 9300\n",
      "Running iter: 9350\n",
      "Running iter: 9400\n",
      "Running iter: 9450\n",
      "Running iter: 9500\n",
      "Running iter: 9550\n",
      "Running iter: 9600\n",
      "Running iter: 9650\n",
      "Running iter: 9700\n",
      "Running iter: 9750\n",
      "Running iter: 9800\n",
      "Running iter: 9850\n",
      "Running iter: 9900\n",
      "Running iter: 9950\n",
      "Total time for 10000: 0.264104127884\n"
     ]
    }
   ],
   "source": [
    "niters = 10000\n",
    "t0 = time.time()\n",
    "min_edges_so_far, min_vertex_sets = run(niters)\n",
    "t1 = time.time()\n",
    "print 'Total time for ' + str(niters) + ': ' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num crossing edges: 113\n",
      "Total time for 10000 iterations: 1.37232826889 hours\n",
      "Number of actual clusters: 10\n"
     ]
    }
   ],
   "source": [
    "print 'Num crossing edges: ' + str(min_edges_so_far)\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ' iterations: ' + str(total/60/60) + ' hours'\n",
    "super_nodes = min_vertex_sets.keys()\n",
    "super_nodes = filter(lambda x: len(min_vertex_sets[x]) > 4, super_nodes)\n",
    "nclusters = len(super_nodes)\n",
    "print 'Number of actual clusters: ' + str(nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "\n",
    "# find the relative frequency for each super node\n",
    "cur.execute(\"SELECT TermVector FROM corpii WHERE Year = '\" + str(year) + \"'\")\n",
    "terms = cur.fetchall()[0][0]\n",
    "\n",
    "nterms = len(terms)\n",
    "overall_frequencies = [0]*nterms\n",
    "cluster_frequencies = [[0]*nterms]*nclusters\n",
    "\n",
    "for cidx, supernode in enumerate(super_nodes):\n",
    "    vertices_in_cluster = min_vertex_sets[supernode]\n",
    "    all_nodes = list(vertices_in_cluster)\n",
    "    all_nodes.append(supernode)\n",
    "    for docid in all_nodes:\n",
    "        cur.execute(\"SELECT TfIdfVector FROM processed_documents WHERE Id = \" + str(docid))\n",
    "        result = cur.fetchone()[0]\n",
    "        tf_idf_vector = [float(x) for x in result]\n",
    "        cluster_frequencies[cidx] = map(sum, izip(cluster_frequencies[cidx], tf_idf_vector))\n",
    "        overall_frequencies = map(sum, izip(overall_frequencies, tf_idf_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 0.8571428571428571, 0.6666666666666666, 0.5, 0.5]\n",
      "\n",
      "['weeks', 'financial-market', 'offsetting', 'manufacturer', 'partially', 'borrowing', 'thought', 'maintain', 'absorb', 'instructions', 'limited', 'portions', 'confidence', 'lenders', 'turned', 'resources', 'pudir', 'regulations', 'consists', 'lowest']\n"
     ]
    }
   ],
   "source": [
    "cluster_freqs_normalized = []\n",
    "for cidx in range(nclusters):\n",
    "    curr_cluster = cluster_frequencies[cidx]\n",
    "    cluster_freqs_normalized.append(\n",
    "        [curr_cluster[i]/overall_frequencies[i] if overall_frequencies[i] > 0 else 0 for i in range(nterms)]\n",
    "    )\n",
    "\n",
    "num_terms = 20\n",
    "curr_cluster = cluster_freqs_normalized[9]\n",
    "# workaround for reversing the array\n",
    "sorted_frequency_idcs = np.argsort(curr_cluster)[::-1]\n",
    "print [curr_cluster[sorted_frequency_idcs[i]] for i in range(num_terms)]\n",
    "print ''\n",
    "print [terms[sorted_frequency_idcs[i]] for i in range(num_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[('The increases in the workweek and employment in April led to notable growth in aggregate hours of production or nonsupervisory workers.',)]\n",
      "\n",
      "[('Business sector debt appeared to have expanded strongly, supported by significant net issuance of U.S. corporate bonds and double-digit growth of business loans at commercial banks.',)]\n",
      "\n",
      "[('Outside the motor vehicle sector, inventories appeared to be well aligned with demand, and surveys indicated that firms continued to be generally comfortable with their level of inventories.',)]\n",
      "\n",
      "[('In these circumstances, the Committee judged that some further policy firming may be needed to keep the risks to the attainment of both sustainable economic growth and price stability roughly in balance but reiterated that it would respond to changes in economic prospects as needed to foster its objectives.',)]\n",
      "\n",
      "[('The staff forecast prepared for this meeting indicated that real GDP growth would slow in the second half of 2006 and 2007, and to a lower rate than had been anticipated in the prior forecast.',)]\n",
      "\n",
      "[('Readings on the growth in the cost of labor were mixed.',)]\n",
      "\n",
      "[('Subsequently, data releases on real activity that were weaker than expected, the Chairman\\xef\\xbf\\xbd s testimony on the semiannual Monetary_Policy_Report, and the release of the June_FOMC minutes all led investors to revise down their expectations for the future path of the federal funds rate.',)]\n",
      "\n",
      "[('Also, the anticipated moderation in aggregate demand implied that pressures on resource utilization likely would not increase and could abate to a degree going forward.',)]\n",
      "\n",
      "[('Still, the overhang of unsold homes remained historically high, and price appreciation of existing homes continued to slow through the second quarter.',)]\n",
      "\n",
      "[('All meeting participants expressed concern about the outlook for inflation.',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters = [[supernode] + list(vertices[supernode]) for supernode in super_nodes]\n",
    "for cluster in clusters:\n",
    "    degrees = [len(vertices[vertex]) for vertex in cluster]\n",
    "    centroid_id = cluster[degrees.index(max(degrees))]\n",
    "    cur.execute(\"SELECT Original FROM processed_documents WHERE Id = \" + str(centroid_id))\n",
    "    centroid = cur.fetchall()\n",
    "    print centroid\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NP', 'PER', 'QUANT', 'abat', 'abil', 'about', 'abov', 'abroad', 'absenc', 'absorb', 'acceler', 'accommod', 'accompani', 'accomplish', 'accord', 'account', 'accumul', 'accur', 'achiev', 'acknowledg', 'across', 'act', 'action', 'activ', 'ad', 'add', 'addit', 'address', 'adequ', 'adjourn', 'adjust', 'adjustable-r', 'administr', 'adopt', 'advanc', 'advers', 'advic', 'advis', 'affect', 'aftermath', 'ag', 'again', 'against', 'agenc', 'agenciar', 'agenda', 'agent', 'aggreg', 'ago', 'agre', 'agreement', 'ahead', 'aim', 'aircraft', 'alert', 'align', 'allow', 'alon', 'along', 'alreadi', 'also', 'alter', 'altern', 'amend', 'amidst', 'among', 'amongst', 'amount', 'ampl', 'amplifi', 'analysi', 'anchor', 'anecdot', 'ani', 'announc', 'anoth', 'anticip', 'apart', 'appar', 'appear', 'appli', 'applic', 'appreci', 'appropri', 'approv', 'ar', 'area', 'aris', 'around', 'arrang', 'articul', 'asid', 'aspect', 'assembl', 'assess', 'asset', 'assign', 'assist', 'associ', 'assum', 'assumpt', 'at', 'attain', 'attend', 'attent', 'attenu', 'attitud', 'attract', 'attribut', 'auction', 'augment', 'augur', 'author', 'auto', 'automak', 'automobil', 'avail', 'aver', 'averag', 'b', 'back', 'backdrop', 'backlog', 'bad', 'balanc', 'balan\\xc3\\xa7a', 'bank', 'bankruptci', 'basa', 'base', 'basi', 'be', 'bear', 'becam', 'becaus', 'becom', 'been', 'befor', 'began', 'begin', 'believ', 'below', 'benefit', 'benign', 'best', 'better', 'between', 'bia', 'bid', 'bill', 'billion', 'bit', 'board', 'bode', 'bolster', 'bond', 'book', 'book-valu', 'boost', 'borrow', 'bring', 'brisk', 'briskli', 'broad', 'broad-bas', 'broad-rang', 'broadli', 'brought', 'budget', 'buffet', 'bui', 'build', 'builder', 'buildup', 'built', 'buoi', 'buoyanc', 'buoyant', 'busi', 'buttress', 'by', 'cabl', 'calcul', 'call', 'can', 'capac', 'capit', 'care', 'carefulli', 'casa', 'case', 'cash', 'categoria', 'caus', 'caution', 'cautiou', 'ceas', 'certain', 'challeng', 'chang', 'charact', 'character', 'check', 'circumst', 'cite', 'civilian', 'claim', 'clariti', 'clearanc', 'clearli', 'climb', 'clip', 'close', 'code', 'cold', 'collect', 'combin', 'come', 'commenc', 'comment', 'commit', 'commod', 'common', 'commonli', 'commun', 'compani', 'compar', 'compens', 'compet', 'competit', 'complet', 'complex', 'complic', 'compon', 'con', 'concentr', 'concern', 'concerndr', 'concert', 'conclus', 'concur', 'concurr', 'condit', 'conduct', 'confid', 'conflict', 'conform', 'connect', 'consecut', 'consequ', 'consid', 'consider', 'consist', 'constitut', 'constrain', 'construct', 'consult', 'consum', 'consumpt', 'contact', 'contain', 'contempl', 'context', 'continu', 'contract', 'contrast', 'contribut', 'control', 'convei', 'cool', 'cooper', 'coordin', 'core', 'corpor', 'correct', 'correctli', 'correspond', 'cost', 'costli', 'could', 'count', 'counter', 'counterbalanc', 'counterpart', 'countri', 'coupl', 'cours', 'cover', 'credit', 'crimp', 'crude', 'cumul', 'currenc', 'curtail', 'curv', 'custom', 'customarili', 'cutback', 'cycl', 'dai', 'damag', 'damp', 'data', 'date', 'dealer', \"dealer'\", 'debt', 'deceler', 'decid', 'decis', 'decisi\\xc3\\xb3', 'declin', 'declinar', 'decreas', 'defer', 'deficit', 'defin', 'definit', 'degre', 'deleteri', 'deliber', 'deliv', 'demand', 'demonstr', 'depend', 'deposit', 'depreci', 'depress', 'descript', 'design', 'desir', 'desira', 'despit', 'destruct', 'detail', 'determin', 'devast', 'develop', 'differ', 'difficulti', 'diminish', 'dip', 'direct', 'directli', 'director', 'disappoint', 'disciplin', 'discontinu', 'discount', 'discuss', 'disloc', 'dismiss', 'disorderli', 'displac', 'disregard', 'disrupt', 'dissatisfi', 'dissent', 'dissip', 'distinct', 'distribut', 'district', 'disturb', 'dividend', 'divis', 'do', 'document', 'dollar', \"dollar'\", 'domest', 'down', 'downshift', 'downsid', 'downward', 'drag', 'drain', 'draw', 'drawback', 'drawn', 'drift', 'drill', 'drive', 'driven', 'driver', 'drop', 'due', 'durabl', 'durat', 'dure', 'e', 'earli', 'earlier', 'earn', 'eas', 'eat', 'eb', 'ebb', 'ebulli', 'echo', 'economi', 'economia', 'economist', \"economy'\", 'edg', 'effect', 'effici', 'effort', 'eighteen', 'elect', 'element', 'elev', 'elicit', 'elimin', 'elsewher', 'embed', 'emerg', 'emerging-market', 'emphas', 'employ', 'employe', 'employee-discount', 'employment-popul', 'encompass', 'encount', 'encourag', 'end', 'energi', 'energy-rel', 'engag', 'enhanc', 'enjoi', 'ensur', 'entail', 'enter', 'environ', 'equip', 'equiti', 'eros', 'error', 'especi', 'essenti', 'establish', 'estat', 'estim', 'euro-area', 'evacue', 'even', 'event', 'eventu', 'evid', 'evidenc', 'evok', 'evolut', 'evolv', 'exampl', 'exceed', 'excess', 'exchang', 'exchange-r', 'exclud', 'execut', 'executiu', 'exert', 'exhibir', 'exhibit', 'exist', 'expand', 'expans', 'expect', 'expenditur', 'expens', 'experi', 'experienc', 'expir', 'explicitli', 'export', 'express', 'expressli', 'extend', 'extens', 'extern', 'extract', 'extraordinarili', 'face', 'facil', 'facilit', 'factor', 'fade', 'failur', 'fairli', 'fall', 'fallar', 'falloff', 'far', 'far-dat', 'faster', 'favor', 'fear', 'fed', 'fee', 'feed', 'fell', 'felt', 'figur', 'fill', 'final', 'financ', 'find', 'firm', 'firmer', 'first', 'first-quart', 'fiscal', 'five', 'fix', 'flat', 'flatten', 'flexibl', 'flow', 'fluctuat', 'flush', 'focus', 'follow', 'food', 'forc', 'forecast', 'foreign', 'form', 'formul', 'forth', 'forward', 'forward-look', 'foster', 'found', 'fourth', 'fourth-quart', 'fourthquart', 'fraction', 'from', 'fuel', 'fulfil', 'full', 'fulli', 'fund', 'fundament', 'further', 'futur', 'ga', 'gain', 'gap', 'garner', 'gasolin', 'gener', 'give', 'given', 'go', 'goal', 'good', 'govern', 'gradual', 'greater', 'grew', 'group', 'grow', 'growth', 'guarante', 'guid', 'had', 'hand', 'hard', 'harm', 'have', 'haver', 'headlin', 'headwind', 'health', 'health-car', 'healthi', 'hear', 'heat', 'heavi', 'height', 'heighten', 'held', 'help', 'henc', 'hi', 'high', 'high-frequ', 'high-tech', 'higher', 'highest', 'highlight', 'hint', 'hire', 'histor', 'hit', 'hold', 'holidai', 'home', 'homebui', 'homebuy', 'horizon', 'hospit', 'hot', 'hour', 'hourli', 'hous', 'household', 'hover', 'how', 'howev', 'hurrican', 'hurricane-affect', 'hurricane-rel', 'i', 'ii', 'imbal', 'immedi', 'impact', 'impart', 'impel', 'impetu', 'implement', 'impli', 'implic', 'implicitli', 'import', 'importantli', 'impos', 'improv', 'in', 'inappropri', 'incentivar', 'includ', 'inclus', 'incom', 'increas', 'increasingli', 'increment', 'index', 'indexar', 'indic', 'indirect', 'individu', 'induc', 'industri', 'industriar', 'inexpens', 'inflat', 'inflation-index', 'inflation-protect', 'influenc', 'influen\\xc3\\xa7a', 'inform', 'infrastructur', 'initi', 'innov', 'input', 'instanc', 'institut', 'instruct', 'insur', 'intend', 'intens', 'intensifi', 'intent', 'interest', 'interest-onli', 'intermedi', 'intermeet', 'interpret', 'into', 'introduc', 'introduct', 'inventori', 'inventory-sal', 'inventory-to-sal', 'invest', 'investment-grad', 'investor', 'involv', 'issu', 'issuanc', 'it', 'item', 'itself', 'job', 'judg', 'judgment', 'jump', 'just', 'keep', 'kei', 'kept', 'labor', 'labor-forc', 'lack', 'lacklust', 'lag', 'landfal', 'languag', 'larg', 'larger', 'larger-than-expect', 'largest', 'last', 'lastli', 'late', 'later', 'latest', 'lead', 'leader', 'leav', 'led', 'left', 'legisl', 'leisur', 'lend', 'lender', 'length', 'lessen', 'lesser', 'level', 'lift', 'light', 'likelihood', 'likewis', 'limit', 'line', 'linger', 'liquid', 'list', 'litr', 'littl', 'loan', 'loan-to-valu', 'long', 'long-expect', 'long-run', 'longer', 'longer-run', 'lose', 'loss', 'lost', 'low', 'lower', 'lowest', 'lure', 'machinist', 'made', 'mai', 'mainli', 'maintain', 'make', 'manag', 'mandat', 'manner', 'manufactur', 'margin', 'mark', 'markedli', 'market', \"market'\", 'market-bas', 'markup', 'materi', 'matter', 'matur', 'maxim', 'maximum', 'mean', 'measur', 'mechan', 'median', 'medium', 'meet', 'member', 'membership', 'mention', 'merchandis', 'met', 'metr', 'mid-decemb', 'mid-novemb', 'mid-octob', 'middle-incom', 'midyear', 'might', 'mildli', 'million', 'mina', 'mind', 'mine', 'minimum', 'minor', 'minu', 'minuta', 'misgiv', 'misinterpret', 'mislead', 'mispric', 'mix', 'moder', 'modestli', 'momentum', 'monei', 'month', 'monthli', 'more-immedi', 'more-subdu', 'mortgag', 'mostli', 'motiv', 'motor', 'move', 'movement', 'multifamili', 'must', 'mute', 'namepl', 'narrow', 'nation', \"nation'\", 'natur', 'near', 'nearli', 'necessarili', 'need', 'neg', 'neighborhood', 'network', 'new', 'next', 'nine', 'noisi', 'non-energi', 'non-high-tech', 'non-market', 'non-oil', 'nonbusi', 'nonemploye', 'nonmarket', 'nonmarket-bas', 'nonoil', 'nontransport', 'northeast', 'not', 'notabl', 'notat', 'note', 'notic', 'notion', 'notwithstand', 'number', 'oath', 'object', 'objectiu', 'oblig', 'observ', 'obsolet', 'occup', 'occur', 'odd', 'of', 'off', 'offer', 'offic', 'offici', 'offset', 'oil', 'on', 'onc', 'one-year', 'ongo', 'onli', 'onset', 'open', 'oper', 'opportun', 'order', 'other', 'otherwis', 'out', 'outcom', 'outdat', 'outlai', 'outlet', 'outlook', 'outpac', 'output', 'outsid', 'outsiz', 'outstand', 'over', 'overal', 'overli', 'overnight', 'overreact', 'overse', 'oversea', 'overvalu', 'ow', 'p', 'pace', 'paper', 'par', 'paragraph', 'pare', 'part', 'parti', 'partial', 'partial-expens', 'particip', 'participar', 'particularli', 'partli', 'partner', 'pass', 'pass-through', 'past', 'path', 'paus', 'payment', 'payrol', 'pend', 'peopl', 'per', 'perceiv', 'percentag', 'percept', 'perform', 'perhap', 'period', 'permit', 'persist', 'person', 'perspectiva', 'petroleum', 'pick', 'pickup', 'pictur', 'pipe', 'pla', 'place', 'plan', 'plant', 'plastic', 'pleasur', 'plenti', 'plu', 'plummet', 'plung', 'plywood', 'point', 'pois', 'polici', 'policymak', 'pondr', 'popul', 'portend', 'porter', 'portfolio', 'portion', 'pose', 'posit', 'possibl', 'post', 'post-hurrican', 'post-katrina', 'potenti', 'power', 'practic', 'pre-hurrican', 'preced', 'preclud', 'predict', 'prefer', 'premium', 'prepar', 'present', 'preserv', 'presid', 'press', 'pressur', 'pressura', 'presum', 'prevail', 'previou', 'previous', 'price', 'price-st', 'primarili', 'prior', 'privat', 'pro', 'probabl', 'problem', 'process', 'produc', 'product', 'profit', 'program', 'progress', 'project', 'prolong', 'promis', 'promot', 'prompt', 'promptli', 'pronounc', 'properti', 'proport', 'propos', 'prospect', 'prove', 'provid', 'provis', 'provisi\\xc3\\xb3', 'public', 'pudir', 'pull', 'pull-back', 'purchas', 'purpos', 'pursuant', 'pursuit', 'push', 'put', 'puzzl', 'qualiti', 'quarter', 'quarter-point', 'question', 'quicken', 'quickli', 'quit', 'rais', 'ran', 'rang', 'rapidli', 'rata', 'rate', 'rather', 'ratifi', 'ratio', 'reach', 'reaction', 'read', 'readili', 'reaffirm', 'reason', 'reassur', 'rebound', 'rebuild', 'reced', 'receipt', 'receiv', 'recent', 'recogn', 'recognit', 'reconstruct', 'record', 'recov', 'reduc', 'reduct', 'refer', 'refin', 'refinanc', 'reflect', 'reform', 'regard', 'region', 'regist', 'regi\\xc3\\xb3', 'regularli', 'reinforc', 'reiter', 'reject', 'rel', 'relat', 'relationship', 'releas', 'relief', 'reloc', 'remain', 'remaind', 'remark', 'remov', 'renew', 'renminbi', 'rent', 'reopen', 'repair', 'repay', 'repeat', 'replac', 'report', 'reportedli', 'repres', 'repurchas', 'request', 'requir', 'reserv', 'resili', 'resolut', 'resourc', 'respect', 'respond', 'respons', 'rest', 'restrain', 'result', 'resum', 'resumpt', 'retail', 'retain', 'retard', 'retent', 'retir', 'retrac', 'retrair', 'retreat', 'retrench', 'return', 'reveal', 'revers', 'review', 'revis', 'revisi\\xc3\\xb3', 'rig', 'rise', 'risen', 'risk', 'risk-tak', 'riskier', 'robustli', 'role', 'rose', 'roughli', 'routin', 'rubber', 'rule', 'run', 'run-up', 'runoff', 'sala', 'salar', 'sap', 'save', 'saw', 'scarciti', 'scatter', 'schedul', 'season', 'second-quart', 'sector', 'secur', 'securit', 'see', 'seek', 'seem', 'seen', 'segment', 'segon', 'select', 'sell', 'semiconductor', 'sens', 'sensit', 'sentenc', 'sentiment', 'seriar', 'serv', 'servic', 'session', 'set', 'sever', 'shall', 'share', 'sharp', 'sharpli', 'shave', 'sheet', 'shift', 'shipbuild', 'shipment', 'shock', 'short', 'shortag', 'shortli', 'should', 'show', 'shown', 'shut-down', 'side', 'sign', 'signal', 'significantli', 'significar', 'simpli', 'sinc', 'singl', 'single-famili', 'situat', 'six', 'size', 'skew', 'skill', 'slack', 'slight', 'slightli', 'slip', 'slow', 'slowdown', 'slower', 'slowli', 'sluggish', 'sluggishli', 'slump', 'small', 'smaller', 'smartli', 'soar', 'soft', 'soften', 'softer', 'softwar', 'sold', 'sole', 'solidli', 'somewhat', 'soon', 'sought', 'sourc', 'space', 'spare', 'speak', 'specif', 'specul', 'speculative-grad', 'spend', 'spike', 'spot', 'spread', 'spring', 'spur', 'squar', 'stabil', 'stability-an', 'staff', \"staff'\", 'stage', 'stai', 'stanc', 'stand', 'standard', 'start', 'state', 'statement', \"statement'\", 'steadi', 'steadili', 'steep', 'steepli', 'stem', 'step', 'step-up', 'still', 'still-accommod', 'still-elev', 'still-low', 'stimulu', 'stir', 'stock', 'stockbuild', 'strategi', 'streamlin', 'strength', 'strengthen', 'stress', 'stretch', 'strike', 'strong', 'stronger', 'stronger-than-expect', 'strongli', 'structur', 'studi', 'sub-par', 'subdu', 'subject', 'submit', 'subpar', 'subsequ', 'subsid', 'substanti', 'subtl', 'success', 'successfulli', 'successor', 'suggest', 'sum', 'summer', 'supersed', 'suppli', 'support', 'sure', 'surg', 'surpris', 'surprisingli', 'surround', 'survei', 'survey-bas', 'sustain', 'swap', 'swing', 'take', 'taken', 'taper', 'target', 'tax', 'technolog', 'temper', 'temporarili', 'tend', 'tendenc', 'tenir', 'tenor', 'tent', 'term', 'termin', 'testimoni', 'text', 'thei', 'them', 'then', 'there', 'therebi', 'therefor', 'thereof', 'thereto', 'thi', 'thin', 'third', 'third-quart', 'thirty-year', 'thought', 'threat', 'threaten', 'three', 'through', 'throughout', 'thu', 'ti', 'tick', 'tighten', 'tighter', 'tilt', 'time', 'to', \"today'\", 'togeth', 'toll', 'too', 'tool', 'total', 'touch', 'toward', 'track', 'trade', 'trade-weight', 'transact', 'transfer', 'transmit', 'transport', 'trend', 'trigger', 'trim', 'troubl', 'truck', 'turn', 'turnaround', 'twelv', 'twelve-month', 'twice', 'two', 'two-month', 'type', 'typic', 'ultim', 'unaffect', 'unanim', 'uncertain', 'uncertainti', 'unchang', 'under', 'underli', 'underpin', 'underscor', 'understand', 'undertak', 'undertaken', 'underwai', 'unemploy', 'uneven', 'unexpect', 'unexpectedli', 'unfortun', 'uniformli', 'unir', 'univers', 'unlik', 'unpreced', 'unseason', 'unsold', 'unusu', 'unutil', 'unwind', 'unwound', 'up', 'upbeat', 'upcom', 'upcreep', 'updat', 'upgrad', 'upon', 'upsid', 'uptick', 'upward', 'us', 'user', 'util', 'vacanc', 'valu', 'valuat', 'vari', 'varieti', 'vehicl', 'veloc', 'versu', 'vi', 'victim', 'view', 'vigor', 'voic', 'volatil', 'volum', 'votar', 'vote', 'wa', 'wage', 'wai', 'wane', 'warrant', 'watch', 'weak', 'weaken', 'weaker', 'weaker-than-expect', 'wealth', 'wealth-to-incom', 'weather', 'week', 'weekli', 'welcom', 'well', 'well-anchor', 'well-balanc', 'well-contain', 'went', 'were', 'who', 'whole', 'wholesal', 'wide', 'widen', 'widespread', 'will', 'willing', 'winter', 'with', 'within', 'without', 'withstand', 'word', 'work', 'worker', 'workforc', 'workweek', 'worri', 'worrisom', 'would', 'year', \"year'\", 'year-ago', 'year-earli', 'yet', 'yield']\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT TermVector FROM corpii WHERE Year = '2005'\")\n",
    "terms2005 = cur.fetchone()[0]\n",
    "print sorted(terms2005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
