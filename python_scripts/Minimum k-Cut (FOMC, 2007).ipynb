{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum k-cut Algorithm\n",
    "\n",
    "### High-level Algorithm Specification:\n",
    "\n",
    "1. Create vertex list and an edges list, e.g.:\n",
    "\n",
    "    ```javascript\n",
    "    vertices = {1: [2,4,5], 2: [3,4,5], 3: [2,4], 4: [1,2,3], 5: [1,2]}\n",
    "    edges = [[1,2], [1,4], [1,5], [2,3], [2,4], [2,5], [3,4]]\n",
    "    ```\n",
    "\n",
    "2. Keep track of the minimum cut so far:\n",
    "\n",
    "    ```javascript\n",
    "    // really this could be the max degree of all vertices, I believe\n",
    "    min_edges_so_far = len(edges)\n",
    "    min_vertex_sets = {1:[], 2:[], 3:[], 4:[], 5:[]}\n",
    "    ```\n",
    "\n",
    "3. *Iterate at least `n^2 log n` times (where n is the original number of vertices)*\n",
    "    \n",
    "    **Intiate:**\n",
    "    \n",
    "    ```javascript\n",
    "    temp_vertex_sets = copy(min_vertex_sets)\n",
    "    temp_vertices = copy(vertices)\n",
    "    temp_edges = copy(edges)\n",
    "    ```\n",
    "\n",
    "    **While num_vertices > k:**\n",
    "\n",
    "    1. Pick an edge at random: the first vertex (`v1`) will absorb the second (`v2`). Add `v2` and `temp_vertex_sets[v2]` to `temp_vertex_sets[v1]` and delete `temp_vertex_sets[v2]`.\n",
    "    2. All vertices adjacent to `v2` are added to `temp_vertices[v1]` unless already present. Remove `v2` from `temp_vertices[v1]`.\n",
    "    3. Replace all instances of `v2` in `temp_edges` with `v1`, unless the other vertex of the edge is itself `v1`. In the latter case, delete the edge (e.g. remove self-loops). **Note:** Parallel edges are allowed; there may be multiple instances of an edge comprised the same vertex pair.\n",
    "\n",
    "    **Finally:** The number of final edges is the number of edges across the final cut in this iteration. If it is less than min_edges_so_far, update `min_edges_so_far = len(temp_edges)` and `min_vertex_sets = temp_vertex_sets`.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Select all measurements and document ids from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2577"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "database = 'fomc'\n",
    "conn = psycopg2.connect(\"dbname=\" + database + \" user=abarciauskas\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "year = 2007\n",
    "cosine_thresh = 0.25\n",
    "cur.execute(\"SELECT Doc1Id,Doc2Id,CosineSimilarity FROM alignments WHERE Year = '\" + str(year) + \"'\"\n",
    "           \" AND CosineSimilarity >= \" + str(cosine_thresh) + \" ORDER BY random() LIMIT 2577\")\n",
    "cosine_sims = cur.fetchall()\n",
    "len(cosine_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the graph\n",
    "\n",
    "The graph is comprised a list of edges (a vertex tuple) and a dictionary of vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1195\n",
      "2577\n"
     ]
    }
   ],
   "source": [
    "def create_graph(alignments):\n",
    "    edges = [tuple([x[0],x[1]]) for x in alignments]\n",
    "    vertices = {}\n",
    "    for edge in edges:\n",
    "        v1 = edge[0]\n",
    "        v2 = edge[1]\n",
    "        if v1 in vertices.keys():\n",
    "            vertices[v1].add(v2)\n",
    "        else:\n",
    "            vertices[v1] = {v2}\n",
    "        if v2 in vertices.keys():\n",
    "            vertices[v2].add(v1)\n",
    "        else:\n",
    "            vertices[v2] = {v1}\n",
    "    return [edges, vertices]\n",
    "\n",
    "edges, vertices = create_graph(cosine_sims)\n",
    "print len(vertices)\n",
    "print len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number vertices fully connected graph: 1140\n"
     ]
    }
   ],
   "source": [
    "# need to find disconnected graphs\n",
    "graphs = []\n",
    "unvisited = set(vertices.keys())\n",
    "\n",
    "#Detect distinct graphs\n",
    "while len(unvisited) > 0:\n",
    "    # for every vertex, find all of its connected components and recurse on those vertices\n",
    "    visited = []\n",
    "    current_vertex = unvisited.pop()\n",
    "    visited.append(current_vertex)\n",
    "    stack_to_visit = list(vertices[current_vertex])\n",
    "    while len(stack_to_visit) > 0:\n",
    "        current_vertex = stack_to_visit.pop()\n",
    "        current_adj_vtcs = vertices[current_vertex]\n",
    "        if current_vertex not in visited: visited.append(current_vertex)\n",
    "        if current_vertex in unvisited: unvisited.remove(current_vertex)        \n",
    "        for v in current_adj_vtcs:\n",
    "            if v not in visited:\n",
    "                stack_to_visit.insert(0, v)\n",
    "    graphs.append(visited)\n",
    "\n",
    "# print len(unvisited)\n",
    "# print len(visited)\n",
    "# print len(graphs)\n",
    "# print ''\n",
    "\n",
    "graph_lengths = [len(graph) for graph in graphs]\n",
    "fc_graph = graphs[graph_lengths.index(max(graph_lengths))]\n",
    "print 'Number vertices fully connected graph: ' + str(len(fc_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1195\n",
      "1140\n",
      "2577\n",
      "2546\n"
     ]
    }
   ],
   "source": [
    "# Remove loner graphs\n",
    "# fc = fully connected\n",
    "set_fc_graph_vertices = set(fc_graph)\n",
    "loners = set_fc_graph_vertices ^ set(vertices.keys())\n",
    "\n",
    "fc_vertices = deepcopy(vertices)\n",
    "fc_edges = deepcopy(edges)\n",
    "\n",
    "print(len(fc_vertices))\n",
    "for loner in loners:\n",
    "    fc_vertices.pop(loner, None)\n",
    "print(len(fc_vertices))\n",
    "\n",
    "print(len(fc_edges))\n",
    "fc_edges = filter(lambda x: not list(x)[0] in loners and not list(x)[1] in loners, fc_edges)    \n",
    "print(len(fc_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Keep track of minimum so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Random iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def run(niters):\n",
    "    min_fc_edges_so_far = len(fc_edges)\n",
    "    min_vertex_sets = {key: set() for key in fc_vertices.keys()}\n",
    "    for iteridx in range(niters):\n",
    "        if iteridx % 50 == 0: print 'Running iter: ' + str(iteridx)        \n",
    "        temp_vertex_sets = {key: set() for key in fc_vertices.keys()}\n",
    "        temp_fc_vertices = deepcopy(fc_vertices)\n",
    "        temp_fc_edges = fc_edges[:]        \n",
    "        while len(temp_fc_vertices) > k:\n",
    "            # pick an edge at random and delete it\n",
    "            rand_idx = int(random.random()*len(temp_fc_edges))\n",
    "            random_edge = temp_fc_edges.pop(rand_idx)\n",
    "            # Add v2 and temp_vertex_sets[v2] to temp_vertex_sets[v1] and delete temp_vertex_sets[v2]\n",
    "            v1 = list(random_edge)[0]\n",
    "            v2 = list(random_edge)[1]\n",
    "            temp_vertex_sets[v1] = temp_vertex_sets[v1].union(temp_vertex_sets[v2])\n",
    "            temp_vertex_sets[v1].add(v2)\n",
    "            temp_vertex_sets.pop(v2, None)\n",
    "\n",
    "            # All fc_vertices adjacent to v2 are added to temp_fc_vertices[v1] unless already present.\n",
    "            # Remove v2 from temp_fc_vertices[v1].\n",
    "            adj_v2 = temp_fc_vertices[v2]\n",
    "            temp_fc_vertices[v1] = temp_fc_vertices[v1].union(adj_v2)\n",
    "            temp_fc_vertices[v1].remove(v2)\n",
    "            temp_fc_vertices.pop(v2, None)\n",
    "\n",
    "            # Replace all instances of v2 in temp_fc_edges with v1, unless the other vertex of the edge is itself v1.\n",
    "            # In the latter case, delete the edge (e.g. remove self-loops).\n",
    "            # Note: Parallel fc_edges are allowed; there may be multiple instances of an edge comprised the same vertex pair.\n",
    "            remove_fc_edges = []\n",
    "            for i,cur_edge in enumerate(temp_fc_edges):\n",
    "                if len(cur_edge) > 1:\n",
    "                    cur_edge_v1 = list(cur_edge)[0]\n",
    "                    cur_edge_v2 = list(cur_edge)[1]\n",
    "                    if (cur_edge == random_edge):\n",
    "                        remove_fc_edges.append(i)\n",
    "                    elif cur_edge_v1 == v2:\n",
    "                        temp_fc_edges[i] = {v1, cur_edge_v2}\n",
    "                        # remove this edge from temp_fc_vertices\n",
    "                        # it may have already been removed because we keep parallel fc_edges around\n",
    "                        if v2 in temp_fc_vertices[cur_edge_v2]: temp_fc_vertices[cur_edge_v2].remove(v2)\n",
    "                    elif cur_edge_v2 == v2:\n",
    "                        temp_fc_edges[i] = {cur_edge_v1, v1}\n",
    "                        # it may have already been removed because we keep parallel fc_edges around\n",
    "                        if v2 in temp_fc_vertices[cur_edge_v1]: temp_fc_vertices[cur_edge_v1].remove(v2)\n",
    "            # work around for delete\n",
    "            temp_fc_edges = [set(i) for j, i in enumerate(temp_fc_edges) if j not in remove_fc_edges]\n",
    "            #Finally: The number of final fc_edges is the number of fc_edges across the final cut in this iteration.\n",
    "            #If it is less than min_fc_edges_so_far, update min_fc_edges_so_far = len(temp_fc_edges) and min_vertex_sets = temp_vertex_sets.\n",
    "            if len(temp_fc_edges) < min_fc_edges_so_far:\n",
    "                min_fc_edges_so_far = len(temp_fc_edges)\n",
    "                min_vertex_sets = temp_vertex_sets\n",
    "    return min_fc_edges_so_far, min_vertex_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iter: 0\n",
      "Total time for 1: 3.62939500809\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "n = len(fc_vertices)\n",
    "niters = 1\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "min_fc_edges_so_far, min_vertex_sets = run(niters)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ': ' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.04082640012\n"
     ]
    }
   ],
   "source": [
    "niters = int(np.ceil(n**2*np.log(n)))\n",
    "niters = 5000\n",
    "total_seconds = niters*total\n",
    "minutes = total_seconds/60\n",
    "hours = minutes/60\n",
    "print hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "niters = 10000\n",
    "t0 = time.time()\n",
    "min_edges_so_far, min_vertex_sets = run(niters)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ': ' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num crossing edges: 126\n",
      "Total time for 10000 iterations: 4.75294949664 hours\n",
      "Number of actual clusters: 2\n"
     ]
    }
   ],
   "source": [
    "print 'Num crossing edges: ' + str(min_edges_so_far)\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ' iterations: ' + str(total/60/60) + ' hours'\n",
    "super_nodes = min_vertex_sets.keys()\n",
    "super_nodes = filter(lambda x: len(min_vertex_sets[x]) > 4, super_nodes)\n",
    "nclusters = len(super_nodes)\n",
    "print 'Number of actual clusters: ' + str(nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "\n",
    "# find the relative frequency for each super node\n",
    "cur.execute(\"SELECT TermVector FROM corpii WHERE Year = '\" + str(year) + \"'\")\n",
    "terms = cur.fetchall()[0][0]\n",
    "\n",
    "nterms = len(terms)\n",
    "overall_frequencies = [0]*nterms\n",
    "cluster_frequencies = [[0]*nterms]*nclusters\n",
    "\n",
    "for cidx, supernode in enumerate(super_nodes):\n",
    "    vertices_in_cluster = min_vertex_sets[supernode]\n",
    "    all_nodes = list(vertices_in_cluster)\n",
    "    all_nodes.append(supernode)\n",
    "    for docid in all_nodes:\n",
    "        cur.execute(\"SELECT TfIdfVector FROM processed_documents WHERE Id = \" + str(docid))\n",
    "        result = cur.fetchone()[0]\n",
    "        tf_idf_vector = [float(x) for x in result]\n",
    "        cluster_frequencies[cidx] = map(sum, izip(cluster_frequencies[cidx], tf_idf_vector))\n",
    "        overall_frequencies = map(sum, izip(overall_frequencies, tf_idf_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b1bb1fbe9a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcurr_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_freqs_normalized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# workaround for reversing the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msorted_frequency_idcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "cluster_freqs_normalized = []\n",
    "for cidx in range(nclusters):\n",
    "    curr_cluster = cluster_frequencies[cidx]\n",
    "    cluster_freqs_normalized.append(\n",
    "        [curr_cluster[i]/overall_frequencies[i] if overall_frequencies[i] > 0 else 0 for i in range(nterms)]\n",
    "    )\n",
    "\n",
    "num_terms = 20\n",
    "curr_cluster = cluster_freqs_normalized[3]\n",
    "# workaround for reversing the array\n",
    "sorted_frequency_idcs = np.argsort(curr_cluster)[::-1]\n",
    "print [curr_cluster[sorted_frequency_idcs[i]] for i in range(num_terms)]\n",
    "print ''\n",
    "print [terms[sorted_frequency_idcs[i]] for i in range(num_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[928, 1024, 1, 4, 5, 6, 8, 24, 25, 26, 28, 33, 36, 37, 38, 49, 50, 51, 52, 58, 60, 61, 62, 66, 67, 72, 75, 76, 78, 79, 80, 81, 85, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 107, 108, 109, 117, 120, 125, 127, 137, 146, 147, 148, 152, 153, 154, 155, 157, 164, 165, 166, 167, 168, 169, 170, 171, 172, 176, 177, 178, 183, 187, 189, 190, 191, 196, 202, 203, 208, 209, 213, 214, 215, 216, 218, 220, 222, 223, 224, 225, 226, 229, 230, 231, 233, 236, 237, 238, 244, 250, 251, 252, 257, 258, 264, 267, 271, 277, 284, 292, 295, 296, 297, 300, 307, 308, 309, 310, 314, 321, 323, 324, 326, 327, 332, 333, 334, 336, 340, 341, 342, 343, 344, 345, 347, 349, 352, 354, 355, 358, 359, 360, 361, 362, 363, 371, 377, 381, 383, 387, 389, 404, 415, 418, 419, 421, 422, 429, 430, 431, 432, 434, 436, 437, 438, 445, 446, 447, 450, 454, 455, 456, 458, 460, 461, 462, 463, 465, 466, 469, 471, 472, 475, 479, 480, 481, 482, 483, 484, 487, 488, 489, 495, 497, 500, 503, 507, 510, 512, 514, 515, 525, 527, 535, 536, 547, 549, 551, 552, 553, 557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 570, 571, 573, 576, 577, 578, 579, 580, 582, 583, 584, 585, 586, 594, 596, 597, 598, 599, 600, 601, 602, 603, 604, 606, 607, 608, 610, 614, 615, 617, 618, 621, 624, 628, 630, 636, 640, 644, 648, 658, 660, 661, 662, 664, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 678, 681, 688, 691, 697, 699, 702, 703, 707, 709, 711, 712, 713, 714, 715, 716, 717, 718, 719, 721, 723, 730, 731, 733, 748, 753, 756, 758, 765, 775, 777, 781, 782, 783, 785, 787, 788, 789, 790, 791, 792, 793, 796, 797, 798, 809, 818, 819, 824, 826, 828, 830, 833, 839, 840, 841, 842, 845, 846, 847, 848, 851, 852, 853, 857, 858, 859, 861, 862, 864, 866, 869, 881, 888, 899, 901, 902, 903, 910, 911, 914, 915, 916, 917, 918, 919, 920, 921, 922, 926, 931, 939, 940, 941, 944, 947, 948, 959, 962, 964, 965, 966, 967, 969, 970, 971, 972, 980, 982, 989, 992, 998, 999, 1000, 1002, 1008, 1010, 1015, 1017, 1019, 1022, 1023]\n",
      "[('In making operating arrangements with foreign central banks on System holdings of foreign currencies, the Federal_Reserve_Bank of New_York shall not commit itself to maintain any specific balance, unless authorized by the Federal_Open_Market_Committee.',)]\n",
      "\n",
      "[('The_U.S. international trade deficit narrowed in May as the value of exports of goods and services rose slightly and the value of imports declined, partly reflecting a sharp drop in the value of oil imports.',)]\n",
      "\n",
      "[('Hiring in the services and construction sectors remained strong, but the manufacturing sector posted further small job losses.',)]\n",
      "\n",
      "[('At the conclusion of the discussion, the Committee voted to authorize and direct the Federal_Reserve_Bank of New_York, until it was instructed otherwise, to execute transactions in the System_Account in accordance with the following domestic policy directive:\" The_Federal_Open_Market_Committee seeks monetary and financial conditions that will foster price stability and promote sustainable growth in output. To further its long-run objectives, the Committee in the immediate future seeks conditions in reserve markets consistent with increasing the federal funds rate to an average of around 4 percent.\" The vote encompassed approval of the paragraph below for inclusion in the statement to be released shortly after the meeting:\" The_Committee perceives that, with appropriate monetary policy action, the upside and downside risks to the attainment of both sustainable growth and price stability should be kept roughly equal. With underlying inflation expected to be contained, the Committee believes that policy accommodation can be removed at a pace that is likely to be measured. Nonetheless, the Committee will respond to changes in economic prospects as needed to fulfill its obligation to maintain price stability.\" Votes for this action: Messrs.',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters = [[supernode] + list(min_vertex_sets[supernode]) for supernode in super_nodes]\n",
    "\n",
    "for cluster in clusters:\n",
    "    degrees = [len(vertices[vertex]) for vertex in cluster]\n",
    "    centroid_id = cluster[degrees.index(max(degrees))]\n",
    "    cur.execute(\"SELECT Original FROM processed_documents WHERE Id = \" + str(centroid_id))\n",
    "    centroid = cur.fetchall()\n",
    "    print centroid\n",
    "    print ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
