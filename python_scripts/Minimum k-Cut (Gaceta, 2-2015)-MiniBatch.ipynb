{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum k-cut Algorithm\n",
    "\n",
    "### High-level Algorithm Specification:\n",
    "\n",
    "1. Create vertex list and an edges list, e.g.:\n",
    "\n",
    "    ```javascript\n",
    "    vertices = {1: [2,4,5], 2: [3,4,5], 3: [2,4], 4: [1,2,3], 5: [1,2]}\n",
    "    edges = [[1,2], [1,4], [1,5], [2,3], [2,4], [2,5], [3,4]]\n",
    "    ```\n",
    "\n",
    "2. Keep track of the minimum cut so far:\n",
    "\n",
    "    ```javascript\n",
    "    // really this could be the max degree of all vertices, I believe\n",
    "    min_edges_so_far = len(edges)\n",
    "    min_vertex_sets = {1:[], 2:[], 3:[], 4:[], 5:[]}\n",
    "    ```\n",
    "\n",
    "3. *Iterate at least `n^2 log n` times (where n is the original number of vertices)*\n",
    "    \n",
    "    **Intiate:**\n",
    "    \n",
    "    ```javascript\n",
    "    temp_vertex_sets = copy(min_vertex_sets)\n",
    "    temp_vertices = copy(vertices)\n",
    "    temp_edges = copy(edges)\n",
    "    ```\n",
    "\n",
    "    **While num_vertices > k:**\n",
    "\n",
    "    1. Pick an edge at random: the first vertex (`v1`) will absorb the second (`v2`). Add `v2` and `temp_vertex_sets[v2]` to `temp_vertex_sets[v1]` and delete `temp_vertex_sets[v2]`.\n",
    "    2. All vertices adjacent to `v2` are added to `temp_vertices[v1]` unless already present. Remove `v2` from `temp_vertices[v1]`.\n",
    "    3. Replace all instances of `v2` in `temp_edges` with `v1`, unless the other vertex of the edge is itself `v1`. In the latter case, delete the edge (e.g. remove self-loops). **Note:** Parallel edges are allowed; there may be multiple instances of an edge comprised the same vertex pair.\n",
    "\n",
    "    **Finally:** The number of final edges is the number of edges across the final cut in this iteration. If it is less than min_edges_so_far, update `min_edges_so_far = len(temp_edges)` and `min_vertex_sets = temp_vertex_sets`.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Select all measurements and document ids from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import random\n",
    "execfile('utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = 'gaceta'\n",
    "conn = psycopg2.connect(\"dbname=\" + database + \" user=abarciauskas\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "year = '2-2015'\n",
    "cosine_thresh = 0.25\n",
    "cur.execute(\"SELECT Doc1Id,Doc2Id,CosineSimilarity FROM alignments WHERE Year = '\" + str(year) + \"'\"\n",
    "           \" AND CosineSimilarity >= \" + str(cosine_thresh) + \" ORDER BY random() LIMIT 200\")\n",
    "cosine_sims = cur.fetchall()\n",
    "len(cosine_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the graph\n",
    "\n",
    "The graph is comprised a list of edges (a vertex tuple) and a dictionary of vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number vertices in complete graph: 220\n",
      "Number edges in complete graph: 200\n"
     ]
    }
   ],
   "source": [
    "edges, vertices = create_graph(cosine_sims)\n",
    "print 'Number vertices in complete graph: ' + str(len(vertices))\n",
    "print 'Number edges in complete graph: ' + str(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number vertices fully connected graph: 143\n"
     ]
    }
   ],
   "source": [
    "# need to find disconnected graphs\n",
    "graphs = build_distinct_graphs(vertices)\n",
    "\n",
    "graph_lengths = [len(graph) for graph in graphs]\n",
    "fc_graph = graphs[graph_lengths.index(max(graph_lengths))]\n",
    "print 'Number vertices fully connected graph: ' + str(len(fc_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices in fully connected graph: 143\n",
      "Edges in fully connected graph: 145\n"
     ]
    }
   ],
   "source": [
    "# Remove loner graphs from the most fully connected graph (fc = fully connected)\n",
    "set_fc_graph_vertices = set(fc_graph)\n",
    "loners = set_fc_graph_vertices ^ set(vertices.keys())\n",
    "\n",
    "fc_vertices = deepcopy(vertices)\n",
    "fc_edges = deepcopy(edges)\n",
    "\n",
    "for loner in loners: fc_vertices.pop(loner, None)\n",
    "print 'Vertices in fully connected graph: ' + str(len(fc_vertices))\n",
    "\n",
    "fc_edges = filter(lambda x: not list(x)[0] in loners and not list(x)[1] in loners, fc_edges)    \n",
    "print 'Edges in fully connected graph: ' + str(len(fc_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 & 3: Keep track of minimum so far and run many random iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "execfile('karger_run.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iter: 0\n",
      "Total time for 1: 0.0308091640472\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n = len(fc_vertices)\n",
    "niters = 1\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "min_fc_edges_so_far, min_vertex_sets = karger_run(niters)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ': ' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868527450694\n"
     ]
    }
   ],
   "source": [
    "niters = int(np.ceil(n**2*np.log(n)))\n",
    "\n",
    "total_seconds = niters*total\n",
    "minutes = total_seconds/60\n",
    "hours = minutes/60\n",
    "print hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iter: 0\n",
      "Running iter: 50\n",
      "Running iter: 100\n",
      "Running iter: 150\n",
      "Running iter: 200\n",
      "Running iter: 250\n",
      "Running iter: 300\n",
      "Running iter: 350\n",
      "Running iter: 400\n",
      "Running iter: 450\n",
      "Running iter: 500\n",
      "Running iter: 550\n",
      "Running iter: 600\n",
      "Running iter: 650\n",
      "Running iter: 700\n",
      "Running iter: 750\n",
      "Running iter: 800\n",
      "Running iter: 850"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "min_edges_so_far, min_vertex_sets = karger_run(niters)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ': ' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num crossing edges: 9\n",
      "Total time for 58027 iterations: -0.139853585296 hours\n",
      "Number of actual clusters: 2\n",
      "Super node of size: 66\n",
      "Super node of size: 29\n"
     ]
    }
   ],
   "source": [
    "print 'Num crossing edges: ' + str(min_edges_so_far)\n",
    "total = t1-t0\n",
    "print 'Total time for ' + str(niters) + ' iterations: ' + str(total/60/60) + ' hours'\n",
    "super_nodes = min_vertex_sets.keys()\n",
    "super_nodes = filter(lambda x: len(min_vertex_sets[x]) >= 3, super_nodes)\n",
    "nclusters = len(super_nodes)\n",
    "print 'Number of actual clusters: ' + str(nclusters)\n",
    "for node in super_nodes: print 'Super node of size: ' + str(len(min_vertex_sets[node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\_\\_\\_\\_\n",
      "\n",
      "**Cluster 1**\n",
      "\n",
      "*Representative:*\n",
      "> At the conclusion of the discussion, the Committee voted to authorize and direct the Federal Reserve Bank of New York, until it was instructed otherwise, to execute transactions in the System Account in accordance with the following domestic policy directive:\" The Federal Open Market Committee seeks monetary and financial conditions that will foster price stability and promote sustainable growth in output. To further its long-run objectives, the Committee in the immediate future seeks conditions in reserve markets consistent with increasing the federal funds rate to an average of around 3-3/4 percent.\" The vote encompassed approval of the paragraph below for inclusion in the statement to be released shortly after the meeting:\" The Committee perceives that, with appropriate monetary policy action, the upside and downside risks to the attainment of both sustainable growth and price stability should be kept roughly equal. With underlying inflation expected to be contained, the Committee believes that policy accommodation can be removed at a pace that is likely to be measured. Nonetheless, the Committee will respond to changes in economic prospects as needed to fulfill its obligation to maintain price stability.\" Votes for this action: Messrs.\n",
      "\n",
      "| Top Terms   |   Relative Frequency |\n",
      "|:------------|---------------------:|\n",
      "| ultim       |                    1 |\n",
      "| fluctuat    |                    1 |\n",
      "| lowest      |                    1 |\n",
      "| ensur       |                    1 |\n",
      "| sharp       |                    1 |\n",
      "| releas      |                    1 |\n",
      "| by          |                    1 |\n",
      "| term        |                    1 |\n",
      "| subdu       |                    1 |\n",
      "| spare       |                    1 |\n",
      "| stretch     |                    1 |\n",
      "| mine        |                    1 |\n",
      "| plummet     |                    1 |\n",
      "| unseason    |                    1 |\n",
      "| combin      |                    1 |\n",
      "| mind        |                    1 |\n",
      "| contain     |                    1 |\n",
      "| set         |                    1 |\n",
      "| categoria   |                    1 |\n",
      "| midyear     |                    1 |\n",
      "\n",
      "\n",
      "\n",
      "\\_\\_\\_\\_\n",
      "\n",
      "**Cluster 2**\n",
      "\n",
      "*Representative:*\n",
      "> Excluding spending on motor vehicles, the growth of real personal consumption expenditures slowed slightly in the second quarter.\n",
      "\n",
      "| Top Terms    |   Relative Frequency |\n",
      "|:-------------|---------------------:|\n",
      "| leader       |                    1 |\n",
      "| neighborhood |                    1 |\n",
      "| achiev       |                    1 |\n",
      "| temporarili  |                    1 |\n",
      "| act          |                    1 |\n",
      "| steepli      |                    1 |\n",
      "| appreci      |                    1 |\n",
      "| notabl       |                    1 |\n",
      "| agenc        |                    1 |\n",
      "| liquid       |                    1 |\n",
      "| next         |                    1 |\n",
      "| evolut       |                    1 |\n",
      "| submit       |                    1 |\n",
      "| focus        |                    1 |\n",
      "| base         |                    1 |\n",
      "| mark         |                    1 |\n",
      "| pull-back    |                    1 |\n",
      "| leisur       |                    1 |\n",
      "| non-energi   |                    1 |\n",
      "| basi         |                    1 |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the relative frequency for each super node\n",
    "execfile('utils.py')\n",
    "cur.execute(\"SELECT TermVector FROM corpii WHERE Year = '\" + str(year) + \"'\")\n",
    "terms = cur.fetchall()[0][0]\n",
    "nterms = len(terms)\n",
    "\n",
    "cluster_frequencies, overall_frequencies = sum_term_frequencies(cur, nterms, nclusters, super_nodes, min_vertex_sets)\n",
    "cluster_frequencies_normalized = normalize_term_frequencies(nterms, cluster_frequencies, overall_frequencies)\n",
    "print_clusters(cur, super_nodes, cluster_frequencies_normalized, 20, terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
